{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210614_4팀_성민지",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt4d3dfeYstU"
      },
      "source": [
        "## 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--dDfa_SX5q6"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
        "print(html.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzqwheWjZRLp"
      },
      "source": [
        "### 요청한 html파일 하나 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33yFR06g7fls"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('https://namu.wiki/RecentChanges')\n",
        "bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "print(bs.h1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsdqnVmAZHO-"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
        "bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "print(bs.h1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYUvoORFfPTY"
      },
      "source": [
        "### 오류 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7llMAvWfN6o"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "\n",
        "try:\n",
        "    html = urlopen(\"https://namu.wiki/RecentChanges\")\n",
        "except HTTPError as e:\n",
        "    print(\"The server returned an HTTP error\")\n",
        "except URLError as e:\n",
        "    print(\"The server could not be found!\")\n",
        "else:\n",
        "    print(html.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVAxvyYH8enN"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "\n",
        "try:\n",
        "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
        "except HTTPError as e:\n",
        "    print(\"The server returned an HTTP error\")\n",
        "except URLError as e:\n",
        "    print(\"The server could not be found!\")\n",
        "else:\n",
        "    print(html.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ablmjxC0lR9-"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def getTitle(url):\n",
        "    try:\n",
        "        html = urlopen(url)\n",
        "    except HTTPError as e:\n",
        "        return None\n",
        "    try:\n",
        "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
        "        title = bsObj.body.h1   #태그명?\n",
        "    except AttributeError as e:\n",
        "        return None\n",
        "    return title\n",
        "\n",
        "\n",
        "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
        "if title == None:\n",
        "    print(\"Title could not be found\")\n",
        "else:\n",
        "    print(title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZxdwQnG9zLZ"
      },
      "source": [
        "#BeautifulSoup\n",
        "#속성을 통해 태그 검색 \n",
        "#태그목록 다루기\n",
        "#트리 내비게이션 분석하기 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR64gpksnzu7"
      },
      "source": [
        "### find() findAll()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgAo7N4rnyzT"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
        "bs = BeautifulSoup(html, \"html.parser\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IOc3OHQn8z9"
      },
      "source": [
        "nameList = bs.findAll('span', {'class': 'green'})   #attribute\n",
        "for name in nameList:\n",
        "    print(name.get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0qu43GGygvH"
      },
      "source": [
        "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
        "print([title for title in titles])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STZvNAbkl-y4"
      },
      "source": [
        "allText = bs.find_all('span', {'class':{'green', 'red'}})\n",
        "print([text for text in allText])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7sujABypjv"
      },
      "source": [
        "nameList = bs.findAll(text=\"the prince\") \n",
        "print(len(nameList)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V5GIBKQUn3l"
      },
      "source": [
        "allText = bs.findAll(id=\"text\") \n",
        "print(allText) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC-GwYX8k92w"
      },
      "source": [
        "# allText = bs.findAll(id=\"text\") 와 동일\n",
        "allText = bs.findAll( '',{'id':\"text\"}) \n",
        "print(allText) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2EMYP-pziCx"
      },
      "source": [
        "allText = bs.findAll(id=\"text\") \n",
        "print(allText[0].get_text()) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg8Drnp5JULm"
      },
      "source": [
        "allText = bs.select_one(\"#text > span\") \n",
        "print(allText) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbKCv5HJLkxp"
      },
      "source": [
        "allText = bs.select_one(\"#text > span\") \n",
        "print(allText.get_text()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KP1TH6ILw69"
      },
      "source": [
        "allText = bs.select(\"#text > span\") \n",
        "for i in allText:\n",
        "    print(i.get_text()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4432ltO3j0Mv"
      },
      "source": [
        "### 트리구조\n",
        "\n",
        "tr 태그는 table 태그의 children\n",
        "descendants 는 몇단계든 그 아래에 존재"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfM_u7Xdmsxj"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "for child in bs.find('table',{'id':'giftList'}).children:   #table 중 아이디가 기프트리스트의 children\n",
        "    print(child)\n",
        "\n",
        "#tr 값을 읽어오는 거야\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evSAPIdOX3K3"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "for child in bs.find('table',{'id':'giftList'}).descendants:\n",
        "    print(child)\n",
        "\n",
        "#descendants 로 읽어오면 children 하고 뭐가 다를까?\n",
        "#children 은 table 바로 밑을 읽어와\n",
        "#descendants 는 그 밑에 전부~"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3EfljUbn_XB"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
        "    print(sibling)\n",
        "\n",
        "#siblings 는 tr 의 동일한 레벨들 읽어오기\n",
        "# td 읽어오기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LvqUTWpKUd"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "print(bs.find('img',\n",
        "              {'src':'../img/gifts/img1.jpg'})\n",
        "      .parent.previous_sibling.get_text())\n",
        "\n",
        "#부모의 사촌형제 찾기래"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc4VhZa9XgYf"
      },
      "source": [
        "### 정규표현식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgNRRT5IpSgM"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "images = bs.find_all('img', {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
        "for image in images: \n",
        "    print(image['src'])\n",
        "\n",
        "#src 는 이미지 주소\n",
        "#*는 아무거나 들어감\n",
        "#이 식에 맞는 애들만 골라주세요"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4L395Zd3W-s"
      },
      "source": [
        "# 문자열 처리와 정규표현식\n",
        "fox = \"tHe qUICk bROWn fOx.\" \n",
        "print(fox.upper()) \n",
        "print(fox.lower())\n",
        "print(fox.title())\n",
        "line = ' this is the content ' \n",
        "print(line.strip()) #시작과 끝의 공백 삭제\n",
        "num = \"000000000000435\"\n",
        "print(num.strip('0')) #시작과 끝의 공백 대신 해당문자 삭제\n",
        "\n",
        "import re  \n",
        "\n",
        "text = \"\"\"100 John    PROF\n",
        "101 James   STUD\n",
        "102 Mac   STUD\"\"\"  \n",
        "\n",
        "print(re.split('\\s+', text))  # 공백으로 split\n",
        "print(re.findall('\\d+',text))  # 숫자 찾기  \n",
        "print(re.findall('[A-Z][a-z]+',text)) # 대소문자 단어\n",
        "print(re.findall('[A-Z]{2,}',text)) # 대문자 단어 {2,} 2번이상 반복\n",
        "message = ' 민원실 전화번호는 02-730-5800 입니다. 또는 111-1111-1111로 연락바랍니다'\n",
        "phone_num = re.compile(r'\\d{2,3}-\\d{3,4}-\\d{4}')\n",
        "for i in phone_num.findall(message):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DEdgoocHDkL"
      },
      "source": [
        "### 네이버 크롤링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn-Jlhl8rpIh"
      },
      "source": [
        "## 원하는 html 요소가 어디있는지 찾아주는 inspector 기능 사용법 \n",
        "구글 개발자 도구를 사용\n",
        "* 크롬 브라우저를 열고 \"https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC\" \n",
        "* F12 버튼을 클릭\n",
        "* 찾고자 하는 요소를 선택\n",
        "* 구글 개발자 도구 상단 가장 왼쪽의 inspector 클릭\n",
        "* 구글 개발자 도구가 찾고자 하는 요소의 html을 찾아줍니다.\n",
        "* 해당 요소 html에 오른쪽 클릭을 한 후 Copy -> Copy Selector 를 선택해 줍니다.\n",
        "* 클립보드에 css 선택자가 복사되었습니다.\n",
        "* 코드에 적용합니다.\n",
        "* 텍스트만 뽑아오고 싶다면 get_text() 함수를 이용하면 됩니다.\n",
        "\n",
        "그리고 f12 버튼을 눌러보세요\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5dZsOZHIhn"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def getText(url):\n",
        "    try:\n",
        "        html = urlopen(url)\n",
        "    except HTTPError as e:\n",
        "        return None\n",
        "    try:\n",
        "        bs = BeautifulSoup(html, 'html.parser')\n",
        "        # text = soup.select_one('#s_content > div.section > ul > li:nth-child(1) > dl > dt > a')\n",
        "        text = bs.select_one('#s_content > div.section > ul > li:nth-of-type(1) > dl > dt > a')\n",
        "    except AttributeError as e:\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "\n",
        "alltext = getText(\"https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC\")\n",
        "if alltext == None:\n",
        "    print(\"Text could not be found\")\n",
        "else:\n",
        "    print(alltext.get_text(),alltext.attrs['href'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4THyupFVB_O"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # alltext = soup.select_one('#s_content > div.section > ul > li:nth-child(1) > dl > dt > a')\n",
        "    alltext = soup.select_one('#s_content > div.section > ul > li:nth-of-type(1) > dl > dt > a')\n",
        "    print(alltext.get_text(),alltext.attrs['href'])\n",
        "else : \n",
        "    print(response.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7GvOgaGdadH"
      },
      "source": [
        "##예시"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvGOvJR3MCDX"
      },
      "source": [
        "타이틀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhxn1_quJm2A"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = 'https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    alltext = soup.find('title')\n",
        "    print('**',alltext.get_text())\n",
        "else : \n",
        "        print(response.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z00JkSwlMFq8"
      },
      "source": [
        "여러건 태그 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDoRJx3MddWC"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = 'https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    atitle = soup.find('title')\n",
        "    ul = soup.select_one('ul.basic1')\n",
        "    for i in ul.select('li > dl > dt > a'):\n",
        "        print('--',i.get_text(),i.attrs['href'])\n",
        "    for i in ul.select('li > dl > dd'):\n",
        "        if ('파이썬' in i.get_text()) and (i.find('b')):\n",
        "            print('**',i.get_text())\n",
        "else : \n",
        "        print(response.status_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piVmsRDYNYHX"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "a = []\n",
        "b = []\n",
        "c = [] \n",
        "url = 'https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    atitle = soup.find('title')\n",
        "    ul = soup.select_one('ul.basic1')\n",
        "    for i in ul.select('li > dl > dt > a'):\n",
        "        print('--',i.get_text(),i.attrs['href'])\n",
        "        a.append(i.get_text())\n",
        "        c.append(i.attrs['href'])\n",
        "    for i in ul.select('li > dl > dd'):\n",
        "        if ('파이썬' in i.get_text()) and (i.find('b')):\n",
        "            print('**',i.get_text())\n",
        "            b.append(i.get_text())\n",
        "else : \n",
        "        print(response.status_code)\n",
        "\n",
        "df = pd.DataFrame(list(zip(a,b,c)), columns = ['질문' , '내용', '주소'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LidKTDTlTPQs"
      },
      "source": [
        "검색어 입력 받기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14aeucovNVSd"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "a = []\n",
        "b = []\n",
        "c = []\n",
        "url = 'https://kin.naver.com/search/list.nhn?query='\n",
        "i_text =input('검색어를 입력하세요:')       #&page={페이지}\n",
        "response = requests.get(url + i_text)     #검색어 받기 두 주소 합친거\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text            #텍스트를 html로 받아서\n",
        "    soup = BeautifulSoup(html, 'html.parser')   #Beautiful soup 에 구문분석\n",
        "    atitle = soup.find('title')\n",
        "    ul = soup.select_one('ul.basic1')\n",
        "    for i in ul.select('li > dl > dt > a'):\n",
        "        print('--',i.get_text(),i.attrs['href'])\n",
        "        a.append(i.get_text())\n",
        "        c.append(i.attrs['href'])\n",
        "    for i in ul.select('li > dl > dd'):\n",
        "        if (i_text in i.get_text()) and (i.find('b')):  #만약 Python 이라는 string 대신 i_text\n",
        "            print('**',i.get_text())        #페이지수 검색어도 무한으로 찾을 수 있어\n",
        "            b.append(i.get_text())\n",
        "   \n",
        "else : \n",
        "        print(response.status_code)\n",
        "\n",
        "df = pd.DataFrame(list(zip(a,b,c)), columns = ['질문' , '내용', '주소'])\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/test.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1srV6_lSrO"
      },
      "source": [
        "# 연습하기\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "a = []\n",
        "b = []\n",
        "c = []\n",
        "url = 'https://kin.naver.com/search/list.nhn?query='\n",
        "page = '&page='\n",
        "i_text =input('검색어를 입력하세요:')       #&page={페이지}\n",
        "p_text = input('페이지 수 입력하세요:')\n",
        "response = requests.get(url + i_text + page + p_text)     #검색어 받기 두 주소 합친거\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text            #텍스트를 html로 받아서\n",
        "    soup = BeautifulSoup(html, 'html.parser')   #Beautiful soup 에 구문분석\n",
        "    atitle = soup.find('title')\n",
        "    ul = soup.select_one('ul.basic1')\n",
        "    for i in ul.select('li > dl > dt > a'):\n",
        "        print('--',i.get_text(),i.attrs['href'])\n",
        "        a.append(i.get_text())\n",
        "        c.append(i.attrs['href'])\n",
        "    for i in ul.select('li > dl > dd'):\n",
        "        if (i_text in i.get_text()) and (i.find('b')):  #만약 Python 이라는 string 대신 i_text\n",
        "            print('**',i.get_text())        #페이지수 검색어도 무한으로 찾을 수 있어\n",
        "            b.append(i.get_text())\n",
        "   \n",
        "else : \n",
        "        print(response.status_code)\n",
        "\n",
        "df = pd.DataFrame(list(zip(a,b,c)), columns = ['질문' , '내용', '주소'])\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/test.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NttC2hnZoWXF"
      },
      "source": [
        "# 연습하기\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "a = []\n",
        "b = []\n",
        "c = []\n",
        "i_text =input('검색어를 입력하세요:')  \n",
        "page =input('페이지를 입력하세요:')  \n",
        "\n",
        "url = f'https://kin.naver.com/search/list.nhn?query={i_text}&page={page}'\n",
        "\n",
        "     #&page={페이지}\n",
        "\n",
        "response = requests.get(url)     #검색어 받기 두 주소 합친거\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html = response.text            #텍스트를 html로 받아서\n",
        "    soup = BeautifulSoup(html, 'html.parser')   #Beautiful soup 에 구문분석\n",
        "    atitle = soup.find('title')\n",
        "    ul = soup.select_one('ul.basic1')\n",
        "    for i in ul.select('li > dl > dt > a'):\n",
        "        print('--',i.get_text(),i.attrs['href'])\n",
        "        a.append(i.get_text())\n",
        "        c.append(i.attrs['href'])\n",
        "    for i in ul.select('li > dl > dd'):\n",
        "        if (i_text in i.get_text()) and (i.find('b')):  #만약 Python 이라는 string 대신 i_text\n",
        "            print('**',i.get_text())        #페이지수 검색어도 무한으로 찾을 수 있어\n",
        "            b.append(i.get_text())\n",
        "   \n",
        "else : \n",
        "        print(response.status_code)\n",
        "\n",
        "df = pd.DataFrame(list(zip(a,b,c)), columns = ['질문' , '내용', '주소'])\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/test.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOPV5YxFkROK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqYhKlGOynYv"
      },
      "source": [
        "<input id=\"input\" type=\"search\" autocomplete=\"off\" spellcheck=\"false\" role=\"combobox\" placeholder=\"Google 검색 또는 URL 입력\" aria-live=\"polite\">\n",
        "#input"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}